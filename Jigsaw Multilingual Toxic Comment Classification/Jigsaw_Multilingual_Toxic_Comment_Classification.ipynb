{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "#This notebook is run on Kaggle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense, Input, LSTM,GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer,AutoModel\n",
    "from tqdm.notebook import tqdm\n",
    "from keras.callbacks import Callback\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'jplu/tf-xlm-roberta-large'\n",
    "# select model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "def encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=True, \n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "     )\n",
    "\n",
    "    return np.array(enc_di['input_ids'],dtype=np.int32),np.array(enc_di['attention_mask'],dtype=np.int32),np.array(enc_di['token_type_ids'],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len):\n",
    "    \n",
    "    input_ids = Input(shape=(max_len,), dtype=np.int32, name='input_ids')\n",
    "    input_mask = Input(shape=(max_len,), dtype=np.int32, name='input_mask')\n",
    "    segment_ids = Input(shape=(max_len,), dtype=np.int32, name='segment_ids')\n",
    "    transformer_output = transformer((input_ids, input_mask, segment_ids))[0]\n",
    "    \n",
    "    #max + mean pooling\n",
    "    gp = GlobalMaxPooling1D()(transformer_output)\n",
    "    ap = GlobalAveragePooling1D()(transformer_output)\n",
    "    stack = concatenate([gp,ap],axis=1)\n",
    "    \n",
    "    output = Dense(1, activation = 'sigmoid')(stack)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, input_mask, segment_ids], outputs=output)\n",
    "    model.compile(Adam(lr=0.2e-5), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acde40ebebc488b8ab13d3dbe5044e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=513.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38047d05b3b4fbd8fcce17592cf84a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keeping the head and the tail is a good way for dealing with long text classification problems, \n",
    "#as indicated in the paper \"How to Fine-Tune BERT for Text Classification\" by Chi et al\n",
    "    \n",
    "    input_df = df.copy()\n",
    "    max_len = head_len + tail_len\n",
    "    input_df['text_head'] = input_df[text_column].apply(lambda x: ' '.join(x.split()[:head_len]))\n",
    "    input_df['text_tail'] = input_df[text_column].apply(lambda x: ' '.join(x.split()[-tail_len:]))\n",
    "    input_df['text_len_1'] = input_df[text_column].apply(lambda x: len(x.split()))\n",
    "    input_df[text_column] = np.where(input_df['text_len_1'] > max_len,\n",
    "                                     input_df['text_head'] + ' ' + input_df['text_tail'],\n",
    "                                     input_df[text_column])\n",
    "    input_df['text_len_2'] = input_df[text_column].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_set():\n",
    "    \n",
    "    cols = ['comment_text', 'toxic']\n",
    "    train = pd.read_csv('../input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es-cleaned.csv')[cols].head(0)\n",
    "\n",
    "    for lang in langs:\n",
    "        train_lang = pd.read_csv(f'../input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-{lang}-cleaned.csv')[cols]\n",
    "        train_lang['lang'] = lang\n",
    "        train_lang_sampled = pd.concat([train_lang.query('toxic==1'),\n",
    "                                        train_lang.query('toxic==0').sample(sum(train_lang.toxic))])\n",
    "        train = train.append(train_lang_sampled)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "test = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/test.csv')\n",
    "langs = list(set(test['lang']))\n",
    "train = make_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 s, sys: 159 ms, total: 10.6 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_head_tail = get_head_tail(train, text_column= 'comment_text',head_len=126, tail_len=126)\n",
    "valid_head_tail = get_head_tail(valid, text_column= 'comment_text', head_len=126, tail_len=126)\n",
    "test_head_tail = get_head_tail(test, text_column= 'content', head_len=126, tail_len=126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 15s, sys: 1.44 s, total: 4min 17s\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = encode(train_head_tail['comment_text'].values, tokenizer, maxlen=MAX_LEN)\n",
    "x_valid = encode(valid_head_tail['comment_text'].values, tokenizer, maxlen=MAX_LEN)\n",
    "x_test = encode(test_head_tail['content'].values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train['toxic'].values\n",
    "y_valid = valid['toxic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 16* strategy.num_replicas_in_sync\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(len(y_train))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef737840e654a56acd99913ab694cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3271420488.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 256, 1024),  559890432   input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 1024)         0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 1024)         0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2048)         0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2049        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 559,892,481\n",
      "Trainable params: 559,892,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "CPU times: user 2min 5s, sys: 39 s, total: 2min 44s\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003/2003 [==============================] - 1051s 525ms/step - auc: 0.9613 - accuracy: 0.8950 - loss: 0.2498 - val_auc: 0.9467 - val_accuracy: 0.8054 - val_loss: 0.3958\n",
      "Epoch 2/2\n",
      "2003/2003 [==============================] - 1016s 507ms/step - auc: 0.9757 - accuracy: 0.9221 - loss: 0.1945 - val_auc: 0.9449 - val_accuracy: 0.8040 - val_loss: 0.4040\n",
      "CPU times: user 3min 37s, sys: 11.3 s, total: 3min 49s\n",
      "Wall time: 37min 29s\n"
     ]
    }
   ],
   "source": [
    "#fit the model using the training set\n",
    "%%time\n",
    "n_steps = train.shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    steps_per_epoch=n_steps,\n",
    "    epochs=EPOCHS,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "62/62 [==============================] - 31s 497ms/step - auc: 0.9417 - accuracy: 0.9031 - loss: 0.2099\n",
      "Epoch 2/4\n",
      "62/62 [==============================] - 31s 496ms/step - auc: 0.9540 - accuracy: 0.9150 - loss: 0.1864\n",
      "Epoch 3/4\n",
      "62/62 [==============================] - 31s 494ms/step - auc: 0.9584 - accuracy: 0.9193 - loss: 0.1778\n",
      "Epoch 4/4\n",
      "62/62 [==============================] - 30s 491ms/step - auc: 0.9644 - accuracy: 0.9277 - loss: 0.1656\n",
      "CPU times: user 8.92 s, sys: 613 ms, total: 9.53 s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "#fit the model using validation set\n",
    "%%time\n",
    "n_steps = valid.shape[0] // BATCH_SIZE\n",
    "train_history_2 = model.fit(\n",
    "    valid_dataset.repeat(),\n",
    "    steps_per_epoch=n_steps,\n",
    "    epochs=4,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995/1995 [==============================] - 110s 55ms/step\n"
     ]
    }
   ],
   "source": [
    "submission['toxic'] = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.012759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.196475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.629261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.015343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.026323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     toxic\n",
       "0   0  0.012759\n",
       "1   1  0.196475\n",
       "2   2  0.629261\n",
       "3   3  0.015343\n",
       "4   4  0.026323"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('checkpoint.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
